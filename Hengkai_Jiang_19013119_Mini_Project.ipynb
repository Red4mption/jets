{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Red4mption/jets/blob/main/Hengkai_Jiang_19013119_Mini_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG6lGBfwppj7"
      },
      "source": [
        "# CNN for Jets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9CdWy5wppkA"
      },
      "source": [
        "Hengkai Jiang 19013119\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odursP1BppkA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMpABqM6ppkC"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### original data file (run the code only if you don't have the data)\n",
        "\n",
        "I have no idea what pt300, pt600, 0_600, 0_1500 mean, but all four files have the same dictionary and shape, so I just use all of them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQcgdqClppkD"
      },
      "outputs": [],
      "source": [
        "# uncomment the code if you need to run it\n",
        "# I comment them so I don't run them everytime I restart my notebook\n",
        "\n",
        "################################################################\n",
        "\n",
        "# download and unzip the data if you don't have it\n",
        "\n",
        "urllib.request.urlretrieve('http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/jetimage/20190920_partial_10k.zip', \"20190920_partial_10k.zip\")\n",
        "with zipfile.ZipFile(\"20190920_partial_10k.zip\",\"r\") as zip_ref:\n",
        "   zip_ref.extractall()\n",
        "\n",
        "###############################################################\n",
        "\n",
        "# load the data locally, change the path according to yours\n",
        "pt300 = np.load(\"./20190920_partial_10k/20190920_pt300.0_600.0_40bins_10k.npz\")\n",
        "pt600 = np.load(\"./20190920_partial_10k/20190920_pt600.0_1500.0_40bins_10k.npz\")\n",
        "pt1500 = np.load(\"./20190920_partial_10k/20190920_pt1500.0_2500.0_40bins_10k.npz\")\n",
        "pt2500 = np.load(\"./20190920_partial_10k/20190920_pt2500.0_5000.0_40bins_10k.npz\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7idDxh-ppkE"
      },
      "source": [
        "The task is to build a CNN using the images to identify W bosons jets and background jets.\n",
        "\n",
        "The data file contains a lot more data than just images of jets above.\n",
        "\n",
        "So I use only the jet images of W bosons and background to build a new file, reducing the amount of data loaded each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6irAO_2hppkE"
      },
      "outputs": [],
      "source": [
        "# uncomment the code if you need to run it\n",
        "# I comment them so I don't run them everytime I restart my notebook\n",
        "\n",
        "# download and load the datafile by running code blocks above.\n",
        "\n",
        "#concatenate all w jets data \n",
        "w_jets_data = np.concatenate(\n",
        "   (pt300['W_jet_images'],pt600['W_jet_images'],pt1500['W_jet_images'],pt2500['W_jet_images']),axis = 0)\n",
        "\n",
        "#concatenate all background jets data\n",
        "background_jets_data =  np.concatenate(\n",
        "   (pt300['QCD_jet_images'],pt600['QCD_jet_images'],pt1500['QCD_jet_images'],pt2500['QCD_jet_images']),axis = 0)\n",
        "\n",
        "#w jets label\n",
        "w_labels = np.vstack((\n",
        "    np.ones(len(w_jets_data)),np.zeros(len(w_jets_data)))).T\n",
        "\n",
        "#background jets label\n",
        "background_labels = np.vstack((\n",
        "    np.zeros(len(background_jets_data)),np.ones(len(background_jets_data)))).T \n",
        "\n",
        "jet_images = np.concatenate((w_jets_data,background_jets_data),axis = 0)\n",
        "jet_labels = np.concatenate((w_labels,background_labels),axis = 0)\n",
        "\n",
        "#shuffle the data without changing correspondence between two arrays\n",
        "indices = np.arange(jet_images.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "jet_images = jet_images[indices]\n",
        "jet_labels = jet_labels[indices]\n",
        "\n",
        "#separate the data into training set, test set and validation set\n",
        "np.savez_compressed(\n",
        "    \"training_set.npz\",images = jet_images[:int(0.7*len(jet_images))], \n",
        "    labels = jet_labels[:int(0.7*len(jet_images))])\n",
        "np.savez_compressed(\n",
        "    \"test_set.npz\",images = jet_images[int(0.7*len(jet_images)):int(0.85*len(jet_images))], \n",
        "    labels = jet_labels[int(0.7*len(jet_images)):int(0.85*len(jet_images))])\n",
        "np.savez_compressed(\n",
        "    \"validation_set.npz\",images = jet_images[int(0.85*len(jet_images)):], \n",
        "    labels = jet_labels[int(0.85*len(jet_images)):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv_bnRphppkF"
      },
      "source": [
        "### New data files (run codes above to generate them)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4raqh_WppkG"
      },
      "source": [
        "I had considered upload the data files online but I don't know if it is appropriate to do so. So, I include the code for generating in case you would like to run it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B34v6yRdppkH"
      },
      "outputs": [],
      "source": [
        "# change the path accordingly, generate the file using code blocks above\n",
        "training_set = np.load(\"training_set.npz\")\n",
        "training_data = training_set['images'].reshape(training_set['images'].shape[0],40,40,1).astype('float32')\n",
        "training_label = training_set['labels']\n",
        "test_set = np.load(\"test_set.npz\")\n",
        "test_data = test_set['images'].reshape(test_set['images'].shape[0],40,40,1).astype('float32')\n",
        "test_label = test_set['labels']\n",
        "validation_set = np.load(\"validation_set.npz\")\n",
        "validation_data = validation_set['images'].reshape(validation_set['images'].shape[0],40,40,1).astype('float32')\n",
        "validation_label = validation_set['labels']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The CNN"
      ],
      "metadata": {
        "id": "oALvcs7K8kn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "\n",
        "# the architecture of the model\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(30,(3,3),input_shape = (40,40,1)))\n",
        "model.add(tf.keras.layers.PReLU())\n",
        "model.add(tf.keras.layers.Conv2D(30,(3,3)))\n",
        "model.add(tf.keras.layers.PReLU())\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.MaxPool2D((2,2)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(200, activation=\"sigmoid\"))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(2))\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "              optimizer = 'adam',metrics=['accuracy'])\n",
        "\n",
        "############################################\n",
        "\n",
        "# load the model \n",
        "# model = tf.keras.models.load_model()"
      ],
      "metadata": {
        "id": "OwW4NrS67oIq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKVeOl8YDCor",
        "outputId": "a3f93786-f9bd-4b31-a230-c93466a980c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 38, 38, 30)        300       \n",
            "                                                                 \n",
            " p_re_lu_6 (PReLU)           (None, 38, 38, 30)        43320     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 36, 36, 30)        8130      \n",
            "                                                                 \n",
            " p_re_lu_7 (PReLU)           (None, 36, 36, 30)        38880     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 36, 36, 30)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 18, 18, 30)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 9720)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 200)               1944200   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 402       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,035,232\n",
            "Trainable params: 2,035,232\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(training_data, training_label,batch_size=100, epochs=20, validation_data=(validation_data,validation_label))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFn1i2QHEDEm",
        "outputId": "c4066824-006e-4967-a536-764d9a86e9fe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5099 - accuracy: 0.7511 - val_loss: 0.5289 - val_accuracy: 0.7351\n",
            "Epoch 2/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5107 - accuracy: 0.7527 - val_loss: 0.5291 - val_accuracy: 0.7395\n",
            "Epoch 3/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5112 - accuracy: 0.7529 - val_loss: 0.5322 - val_accuracy: 0.7368\n",
            "Epoch 4/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5132 - accuracy: 0.7512 - val_loss: 0.5376 - val_accuracy: 0.7281\n",
            "Epoch 5/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5150 - accuracy: 0.7479 - val_loss: 0.5348 - val_accuracy: 0.7337\n",
            "Epoch 6/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5123 - accuracy: 0.7511 - val_loss: 0.5282 - val_accuracy: 0.7364\n",
            "Epoch 7/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5121 - accuracy: 0.7509 - val_loss: 0.5397 - val_accuracy: 0.7307\n",
            "Epoch 8/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5064 - accuracy: 0.7549 - val_loss: 0.5345 - val_accuracy: 0.7320\n",
            "Epoch 9/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5081 - accuracy: 0.7506 - val_loss: 0.5285 - val_accuracy: 0.7368\n",
            "Epoch 10/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5084 - accuracy: 0.7534 - val_loss: 0.5306 - val_accuracy: 0.7370\n",
            "Epoch 11/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5077 - accuracy: 0.7538 - val_loss: 0.5370 - val_accuracy: 0.7324\n",
            "Epoch 12/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5099 - accuracy: 0.7526 - val_loss: 0.5350 - val_accuracy: 0.7344\n",
            "Epoch 13/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5058 - accuracy: 0.7558 - val_loss: 0.5385 - val_accuracy: 0.7293\n",
            "Epoch 14/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5055 - accuracy: 0.7558 - val_loss: 0.5370 - val_accuracy: 0.7306\n",
            "Epoch 15/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5082 - accuracy: 0.7553 - val_loss: 0.5412 - val_accuracy: 0.7274\n",
            "Epoch 16/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5027 - accuracy: 0.7569 - val_loss: 0.5304 - val_accuracy: 0.7320\n",
            "Epoch 17/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5063 - accuracy: 0.7542 - val_loss: 0.5331 - val_accuracy: 0.7320\n",
            "Epoch 18/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5086 - accuracy: 0.7538 - val_loss: 0.5328 - val_accuracy: 0.7390\n",
            "Epoch 19/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5073 - accuracy: 0.7539 - val_loss: 0.5290 - val_accuracy: 0.7372\n",
            "Epoch 20/20\n",
            "560/560 [==============================] - 17s 30ms/step - loss: 0.5058 - accuracy: 0.7559 - val_loss: 0.5331 - val_accuracy: 0.7377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG1PEbwkppkJ",
        "outputId": "2106f39c-6b91-41dd-a909-ca78e860ef7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.6408880239999917\n",
            "GPU (s):\n",
            "0.05386709199999018\n",
            "GPU speedup over CPU: 67x\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5BYVBuhppkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016e9430-a797-47d0-cec6-98882b3bf4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "arr = np.vstack((np.zeros(3),np.ones(3))).T\n",
        "arr2 = np.vstack((np.ones(3),np.zeros(3))).T\n",
        "print(np.concatenate((arr,arr2),axis = 0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Hengkai Jiang 19013119 Mini Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}